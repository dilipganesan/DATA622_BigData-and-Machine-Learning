This folder has files for Home Work 4. 

mapper.py
reducer.py
hw4output.txt
Hadoop_File.png(Screenshot of my hadoop install)

Submit your answers by modifying this README.md file.

(1 points) What is Hadoop 1's single point of failure and why is this critical? How is this alleviated in Hadoop 2?
Ans : The single point of failure in Hadoop v1 is NameNode. If NameNode fail's then whole Hadoop cluster will not work. 
Since NameNode is the only  point of contact to all DataNodes and if the NameNode fails all communication will stop if it fails.
With HDFS High Availability of Namenode is introduced with Hadoop 2. In this two separate machines are getting configured as 
NameNodes, where one NameNode always in working state and anther is in standby. The Master-Slave configuration was introduced to 
solve this issue.


(2 points) What happens when a data node fails?
The NameNode and DataNode acst as buddie pair. The keep in constant touch with each other, where in datanode keeps sending signals
making sure the datanode is alive. When a NameNode does not receives the signal from the datanode, it declares it as lost. 
To keep the replication of the all the data to defined replication factor, the Namenode will make the copies on other data nodes.
When a data node comes back with all its data, Name node will clean the extra copies of data. 

(1 point) What is a daemon? Describe the role task trackers and job trackers play in the Hadoop environment.
Deamons are processes that run in a background in Hadoop system. They are NameNode, DataNode, ResourceManager and NodeManager.

Job Tracker:  
NameNode and DataNodes store data on HDFS. Processing of data can be done using MapReduce algorithm. 
MapReduce engine sends the code across DataNodes, creating jobs. These jobs are to be continuously monitored by Job tracker 
who manages those jobs.

Task Tracker: A TaskTracker is a node in the cluster that accepts tasks. These tasks are then
assigned to Job Tracker which perform Map, Reduce and Shuffle operations.

(1 point) Why is Cloudera's VM considered pseudo distributed computing? How is it different from a true Hadoop 
cluster computing?
Ans : In general Hadoop installation is standalone, as a single Java process(single JVM Instance) 
      A single node setup is one where you have one datanode and one tasktracker on a  machine.

CDH on the other hand is a pseudo-distributed setup is where you have multiple datanodes and many tasktrackers on a single machine. 
So you have multiple instances of a datanode service running on a single machine to emulate a multi-node cluster.

(1 point) What is Hadoop streaming? What is the Hadoop Ecosystem?
Ans : Hadoop Streaming is the API which helps writing Mapper and Reducers in any programming language.

Hadoop Ecosystem contains Hadoop Distributed File System, MapReduce, YARN, Hive, Pig, Hbase, Flume.


(1 point) During a reducer job, why do we need to know the current key, current value, previous key, and 
cumulative value, but NOT the previous value?

Ans : Reducer job has input which is shuffled and sorted based on the Key. Key is the important parameter for Reducer. 
The Previous Value is not a key and this keeps changing. The previous value is already stored in cumulative value and the 
value changes as and when the key changes.


(3 points) A large international company wants to use Hadoop MapReduce to calculate the # of sales by location by day. 
The logs data has one entry per location per day per sale. Describe how MapReduce will work in this scenario, 
using key words like: intermediate records, shuffle and sort, mappers, reducers, sort, key/value, task tracker, job tracker.

Ans : Job tracker is the master which runs the programs, maintain tasks etc. Job tracker will work is conjunction with 
task tracker. Mapper will read the data and split the data in to Key/Value pairs. In this case the Location will be the key 
and it will keep putting the sale count as values. The output from the mapper is intermediate record, which is then 
shuffled and sorted and fed to the reducer. The part of reducer is to take the key and do the calculation of sales for each 
location and create an output file. The output will be number of sales by location on a day.

